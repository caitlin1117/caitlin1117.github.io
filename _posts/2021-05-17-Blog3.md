---
layout: post
title: Blog3
---

### When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?

We will answer this question by looking at three different machine learning models. We will create these neural network models using the Tensorflow package. A neural network is a series of algorithms that tries to recognize underlying relationships in a set of data. The networks are composed of several layers. Each layer has many units where computation happens. The unit multiplies the input data with a set of weights which is the process of assigning significance to inputs with regard to the task the algorithm is trying to learn. These input-weight products are summed and then the sum is passed through a activation function which determines to what extent the information should progress further through the network to affect the ultimate result.


```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

# requires update to tensorflow 2.4
# >>> conda activate PIC16B
# >>> pip install tensorflow==2.4
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"

train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
```

We first take a look at the dataset. The title column is the title of the article and the text column is the full article text. The fake column is 0 if the article is true and 1 if the article contains fake news.


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.


We need to first take away the stopwords, such as "it, the, is, of, to". Then we need to create a dataset with two inputs and one output. The inputs are title and text, and the output is the fake column.


```python
stop = stopwords.words('english')
def make_dataset(df):
  '''
  Turn the texts into lowercase letters and remove punctuations 
  in order to fully remove the stopwords. 
  Make the dataset and batch the dataset 
  Return the batched dataset

  '''
  for OneOfCols in ["text", "title"]:
    df[OneOfCols] = df[OneOfCols].apply(lambda sentence: ' '.join([word.lower() for word in re.split(r'\W+', sentence) if word.lower() not in stop]))


  data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : df[["title"]], 
            "text" : df["text"]
        }, 
        {
            "fake" : df[["fake"]]
        }
    )
    )
  
  data = data.batch(100)
  return data
   

```


{::options parse_block_html="true" /}
<div class="got-help">
I learned that we can remove punctuation, stopwords and turn words into lower case in a very efficient way.
</div>
{::options parse_block_html="false" /}



```python
data = make_dataset(df)
```

Now we set 80% as training data and 20% as validation data.


```python
data = data.shuffle(buffer_size = len(data))
train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size)
len(train), len(val)
```




    (180, 45)



{::options parse_block_html="true" /}
<div class="gave-help">
I reminded one of my peer that we are using separate testing data, so we don't need to save 10 percent data for testing.
</div>
{::options parse_block_html="false" /}

Next we perform vectorization. Vectorization is the process of representing text as a vector. Here, we'll replace each word by its frequency rank in the data. Each entry in the vector is an integer that represents the frequency rank. For example the number 200 represents the word that is the 200th most common word in the data set. The output sequence length is the length of the output vector. The output will have its time dimension padded with 0 or truncated to exactly output_sequence_length values.

###Model 1

In the first model, we only consider the title.


```python
size_vocabulary = 2000 # only the top distinct words will be tracked

vectorize_layer = TextVectorization(
    max_tokens=size_vocabulary, 
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer.adapt(train.map(lambda x, y: x["title"]))#each title will be a vector of length 500
```

Now we specify the title input as the predictor data. 
The shape parameter is the shape of a single item of data. The title column contains just one entry, so the shape is (1,) (a tuple of length 1). We set the name to "title" which will be used later.
The dtype specifies the type of data contained in the input tensors. In this case, the title is a string.


```python
# title input
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

```

Now we construct the first model. The model has a vectorization layer, a embedding layer and two dropout layers and a dense layer. What is a embedding layer? Ideally, an embedding captures some of the relationship of the input by placing related inputs close together in the embedding space. We asign each distinct word a point in d dimensions. Because the embedded points get updated during the training process of the neural network, we can find what words are related in a multidimensional space. The fisrt argument size_vocabulary is the number of distinct words in the training set. The second argument (10) indicates the dimension of the embedding vectors. The dropout layers are used to prevent overfitting. 

{::options parse_block_html="true" /}
<div class="got-help">
I learned from the feedback that I should explain more about the embedding layer.
</div>
{::options parse_block_html="false" /}



```python
title_features = vectorize_layer(title_input)
embedding = layers.Embedding(size_vocabulary, 10, name = "embedding")
title_features = embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
output = layers.Dense(2, name = "fake")(title_features)
```

{::options parse_block_html="true" /}
<div class="gave-help">
I reminded some of my peers that we should use the same embedding layer and use higher dimensions like 10 instead of 3.
</div>
{::options parse_block_html="false" /}

```python
model = keras.Model(
    inputs = [title_input],
    outputs = output
)
```


```python
model.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization (TextVect (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout (Dropout)            (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 10)                0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________



```python
keras.utils.plot_model(model)
```




    
![png](/images/Untitled1_files/Untitled1_21_0.png)
    




```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = True)
```

    Epoch 1/50


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning:
    
    Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
    


    180/180 [==============================] - 5s 7ms/step - loss: 0.6916 - accuracy: 0.5249 - val_loss: 0.6912 - val_accuracy: 0.5206
    Epoch 2/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.6876 - accuracy: 0.5509 - val_loss: 0.6785 - val_accuracy: 0.6218
    Epoch 3/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.6391 - accuracy: 0.7290 - val_loss: 0.5657 - val_accuracy: 0.8553
    Epoch 4/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.4680 - accuracy: 0.8526 - val_loss: 0.3708 - val_accuracy: 0.8840
    Epoch 5/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.3264 - accuracy: 0.8859 - val_loss: 0.2783 - val_accuracy: 0.9044
    Epoch 6/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.2543 - accuracy: 0.9065 - val_loss: 0.2210 - val_accuracy: 0.9176
    Epoch 7/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.2203 - accuracy: 0.9186 - val_loss: 0.2025 - val_accuracy: 0.9198
    Epoch 8/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1985 - accuracy: 0.9255 - val_loss: 0.1718 - val_accuracy: 0.9327
    Epoch 9/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1831 - accuracy: 0.9277 - val_loss: 0.1723 - val_accuracy: 0.9353
    Epoch 10/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1728 - accuracy: 0.9346 - val_loss: 0.1544 - val_accuracy: 0.9402
    Epoch 11/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1627 - accuracy: 0.9362 - val_loss: 0.1441 - val_accuracy: 0.9449
    Epoch 12/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1569 - accuracy: 0.9396 - val_loss: 0.1406 - val_accuracy: 0.9447
    Epoch 13/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1500 - accuracy: 0.9416 - val_loss: 0.1464 - val_accuracy: 0.9429
    Epoch 14/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1470 - accuracy: 0.9429 - val_loss: 0.1386 - val_accuracy: 0.9449
    Epoch 15/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1439 - accuracy: 0.9431 - val_loss: 0.1336 - val_accuracy: 0.9498
    Epoch 16/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1363 - accuracy: 0.9472 - val_loss: 0.1306 - val_accuracy: 0.9503
    Epoch 17/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1354 - accuracy: 0.9465 - val_loss: 0.1142 - val_accuracy: 0.9538
    Epoch 18/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1338 - accuracy: 0.9486 - val_loss: 0.1223 - val_accuracy: 0.9527
    Epoch 19/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1303 - accuracy: 0.9488 - val_loss: 0.1175 - val_accuracy: 0.9535
    Epoch 20/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1332 - accuracy: 0.9475 - val_loss: 0.1190 - val_accuracy: 0.9536
    Epoch 21/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1287 - accuracy: 0.9502 - val_loss: 0.1240 - val_accuracy: 0.9518
    Epoch 22/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1234 - accuracy: 0.9524 - val_loss: 0.1182 - val_accuracy: 0.9516
    Epoch 23/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1201 - accuracy: 0.9541 - val_loss: 0.1060 - val_accuracy: 0.9604
    Epoch 24/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1195 - accuracy: 0.9546 - val_loss: 0.1186 - val_accuracy: 0.9548
    Epoch 25/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1202 - accuracy: 0.9538 - val_loss: 0.1215 - val_accuracy: 0.9529
    Epoch 26/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1184 - accuracy: 0.9543 - val_loss: 0.1045 - val_accuracy: 0.9598
    Epoch 27/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1190 - accuracy: 0.9555 - val_loss: 0.1313 - val_accuracy: 0.9462
    Epoch 28/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1183 - accuracy: 0.9553 - val_loss: 0.1034 - val_accuracy: 0.9611
    Epoch 29/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1199 - accuracy: 0.9540 - val_loss: 0.0989 - val_accuracy: 0.9616
    Epoch 30/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1140 - accuracy: 0.9563 - val_loss: 0.1040 - val_accuracy: 0.9618
    Epoch 31/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1140 - accuracy: 0.9562 - val_loss: 0.1035 - val_accuracy: 0.9616
    Epoch 32/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1098 - accuracy: 0.9577 - val_loss: 0.1145 - val_accuracy: 0.9571
    Epoch 33/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1122 - accuracy: 0.9558 - val_loss: 0.1212 - val_accuracy: 0.9493
    Epoch 34/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1082 - accuracy: 0.9581 - val_loss: 0.0946 - val_accuracy: 0.9640
    Epoch 35/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1099 - accuracy: 0.9577 - val_loss: 0.1057 - val_accuracy: 0.9611
    Epoch 36/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1089 - accuracy: 0.9575 - val_loss: 0.0972 - val_accuracy: 0.9627
    Epoch 37/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1163 - accuracy: 0.9556 - val_loss: 0.0941 - val_accuracy: 0.9638
    Epoch 38/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1097 - accuracy: 0.9577 - val_loss: 0.0853 - val_accuracy: 0.9667
    Epoch 39/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1043 - accuracy: 0.9585 - val_loss: 0.0960 - val_accuracy: 0.9638
    Epoch 40/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1046 - accuracy: 0.9592 - val_loss: 0.1007 - val_accuracy: 0.9613
    Epoch 41/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1052 - accuracy: 0.9596 - val_loss: 0.1052 - val_accuracy: 0.9564
    Epoch 42/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1106 - accuracy: 0.9567 - val_loss: 0.0956 - val_accuracy: 0.9644
    Epoch 43/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1066 - accuracy: 0.9585 - val_loss: 0.0879 - val_accuracy: 0.9678
    Epoch 44/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1081 - accuracy: 0.9583 - val_loss: 0.0842 - val_accuracy: 0.9700
    Epoch 45/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1036 - accuracy: 0.9611 - val_loss: 0.0855 - val_accuracy: 0.9699
    Epoch 46/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1001 - accuracy: 0.9612 - val_loss: 0.1131 - val_accuracy: 0.9537
    Epoch 47/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1073 - accuracy: 0.9596 - val_loss: 0.0986 - val_accuracy: 0.9580
    Epoch 48/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.0988 - accuracy: 0.9619 - val_loss: 0.0895 - val_accuracy: 0.9671
    Epoch 49/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1032 - accuracy: 0.9608 - val_loss: 0.1240 - val_accuracy: 0.9528
    Epoch 50/50
    180/180 [==============================] - 1s 6ms/step - loss: 0.1026 - accuracy: 0.9613 - val_loss: 0.0872 - val_accuracy: 0.9702



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f6e80034910>




    
![png](/images/Untitled1_files/Untitled1_24_1.png)
    


This model is able to score approximately 96% validation accuracy. We can already get good results just from looking at the title. We see from the above graph that the validation accuracy is slightly higher that the training accuracy. This is due to the fact that we have dropout layers in the model. During training, the algorithm dropped out 20% of the units which prevents overfitting. But these units are used during validation, so it can happen that the validation scores can be higher.

###Model 2

Now we create the second model that only uses the text which follows the same process as before. We use the same vectorization and embedding layer as before.


```python
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
# inputs

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```


```python
text_features = vectorize_layer(text_input)
text_features = embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
output = layers.Dense(2, name = "fake")(text_features)
```


```python
model = keras.Model(
    inputs = [text_input],
    outputs = output
)
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
history = model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = True)
```

    Epoch 1/50


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:591: UserWarning:
    
    Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
    


    180/180 [==============================] - 3s 12ms/step - loss: 0.6828 - accuracy: 0.5686 - val_loss: 0.6554 - val_accuracy: 0.6749
    Epoch 2/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.5239 - accuracy: 0.7967 - val_loss: 0.3243 - val_accuracy: 0.9316
    Epoch 3/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.2728 - accuracy: 0.9116 - val_loss: 0.1974 - val_accuracy: 0.9447
    Epoch 4/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1983 - accuracy: 0.9362 - val_loss: 0.1535 - val_accuracy: 0.9580
    Epoch 5/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1676 - accuracy: 0.9460 - val_loss: 0.1248 - val_accuracy: 0.9631
    Epoch 6/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1449 - accuracy: 0.9545 - val_loss: 0.1179 - val_accuracy: 0.9688
    Epoch 7/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1248 - accuracy: 0.9621 - val_loss: 0.1128 - val_accuracy: 0.9704
    Epoch 8/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1156 - accuracy: 0.9656 - val_loss: 0.1026 - val_accuracy: 0.9709
    Epoch 9/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1080 - accuracy: 0.9667 - val_loss: 0.0900 - val_accuracy: 0.9751
    Epoch 10/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0998 - accuracy: 0.9716 - val_loss: 0.0877 - val_accuracy: 0.9771
    Epoch 11/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0940 - accuracy: 0.9706 - val_loss: 0.0717 - val_accuracy: 0.9811
    Epoch 12/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0857 - accuracy: 0.9754 - val_loss: 0.0779 - val_accuracy: 0.9782
    Epoch 13/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0859 - accuracy: 0.9755 - val_loss: 0.0625 - val_accuracy: 0.9833
    Epoch 14/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0784 - accuracy: 0.9777 - val_loss: 0.0589 - val_accuracy: 0.9871
    Epoch 15/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0774 - accuracy: 0.9773 - val_loss: 0.0641 - val_accuracy: 0.9813
    Epoch 16/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0753 - accuracy: 0.9790 - val_loss: 0.0618 - val_accuracy: 0.9856
    Epoch 17/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0732 - accuracy: 0.9798 - val_loss: 0.0559 - val_accuracy: 0.9862
    Epoch 18/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0715 - accuracy: 0.9791 - val_loss: 0.0473 - val_accuracy: 0.9880
    Epoch 19/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0665 - accuracy: 0.9803 - val_loss: 0.0513 - val_accuracy: 0.9889
    Epoch 20/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0652 - accuracy: 0.9813 - val_loss: 0.0518 - val_accuracy: 0.9891
    Epoch 21/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0646 - accuracy: 0.9807 - val_loss: 0.0470 - val_accuracy: 0.9878
    Epoch 22/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0616 - accuracy: 0.9823 - val_loss: 0.0516 - val_accuracy: 0.9884
    Epoch 23/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0607 - accuracy: 0.9824 - val_loss: 0.0427 - val_accuracy: 0.9871
    Epoch 24/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0557 - accuracy: 0.9848 - val_loss: 0.0481 - val_accuracy: 0.9863
    Epoch 25/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0551 - accuracy: 0.9840 - val_loss: 0.0412 - val_accuracy: 0.9912
    Epoch 26/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0577 - accuracy: 0.9845 - val_loss: 0.0464 - val_accuracy: 0.9876
    Epoch 27/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0576 - accuracy: 0.9819 - val_loss: 0.0371 - val_accuracy: 0.9899
    Epoch 28/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0537 - accuracy: 0.9838 - val_loss: 0.0494 - val_accuracy: 0.9867
    Epoch 29/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0507 - accuracy: 0.9855 - val_loss: 0.0350 - val_accuracy: 0.9902
    Epoch 30/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0538 - accuracy: 0.9837 - val_loss: 0.0304 - val_accuracy: 0.9918
    Epoch 31/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0516 - accuracy: 0.9853 - val_loss: 0.0450 - val_accuracy: 0.9903
    Epoch 32/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0467 - accuracy: 0.9866 - val_loss: 0.0323 - val_accuracy: 0.9922
    Epoch 33/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0487 - accuracy: 0.9850 - val_loss: 0.0420 - val_accuracy: 0.9900
    Epoch 34/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0474 - accuracy: 0.9862 - val_loss: 0.0339 - val_accuracy: 0.9907
    Epoch 35/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0462 - accuracy: 0.9861 - val_loss: 0.0323 - val_accuracy: 0.9916
    Epoch 36/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0455 - accuracy: 0.9863 - val_loss: 0.0295 - val_accuracy: 0.9931
    Epoch 37/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0431 - accuracy: 0.9873 - val_loss: 0.0249 - val_accuracy: 0.9936
    Epoch 38/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0446 - accuracy: 0.9863 - val_loss: 0.0335 - val_accuracy: 0.9920
    Epoch 39/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0451 - accuracy: 0.9874 - val_loss: 0.0265 - val_accuracy: 0.9953
    Epoch 40/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0382 - accuracy: 0.9887 - val_loss: 0.0322 - val_accuracy: 0.9942
    Epoch 41/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0420 - accuracy: 0.9877 - val_loss: 0.0283 - val_accuracy: 0.9940
    Epoch 42/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0404 - accuracy: 0.9877 - val_loss: 0.0293 - val_accuracy: 0.9933
    Epoch 43/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0387 - accuracy: 0.9888 - val_loss: 0.0263 - val_accuracy: 0.9944
    Epoch 44/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0401 - accuracy: 0.9886 - val_loss: 0.0252 - val_accuracy: 0.9933
    Epoch 45/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0395 - accuracy: 0.9875 - val_loss: 0.0199 - val_accuracy: 0.9956
    Epoch 46/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0365 - accuracy: 0.9894 - val_loss: 0.0303 - val_accuracy: 0.9924
    Epoch 47/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0356 - accuracy: 0.9895 - val_loss: 0.0238 - val_accuracy: 0.9949
    Epoch 48/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0376 - accuracy: 0.9887 - val_loss: 0.0242 - val_accuracy: 0.9949
    Epoch 49/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0354 - accuracy: 0.9892 - val_loss: 0.0275 - val_accuracy: 0.9940
    Epoch 50/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 0.0214 - val_accuracy: 0.9964



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f6e2ae08c90>




    
![png](/images/Untitled1_files/Untitled1_31_1.png)
    


We see the result is better than the result of the first model. The validation score reached 99. 

###Model 3

Next we create the third model which uses both the title and the text.


```python
main = layers.concatenate([title_features, text_features], axis = 1)
```


```python
output = layers.Dense(2, name = "fake")(main)
```


```python
model = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
model.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization (TextVectori (None, 500)          0           title[0][0]                      
                                                                     text[0][0]                       
    __________________________________________________________________________________________________
    embedding (Embedding)           (None, 500, 10)      20000       text_vectorization[0][0]         
                                                                     text_vectorization[1][0]         
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 500, 10)      0           embedding[0][0]                  
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 500, 10)      0           embedding[1][0]                  
    __________________________________________________________________________________________________
    global_average_pooling1d (Globa (None, 10)           0           dropout[0][0]                    
    __________________________________________________________________________________________________
    global_average_pooling1d_1 (Glo (None, 10)           0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 10)           0           global_average_pooling1d[0][0]   
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 10)           0           global_average_pooling1d_1[0][0] 
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 32)           352         dropout_1[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 32)           352         dropout_3[0][0]                  
    __________________________________________________________________________________________________
    concatenate (Concatenate)       (None, 64)           0           dense[0][0]                      
                                                                     dense_1[0][0]                    
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            130         concatenate[0][0]                
    ==================================================================================================
    Total params: 20,834
    Trainable params: 20,834
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
keras.utils.plot_model(model)
```




    
![png](/images/Untitled1_files/Untitled1_38_0.png)
    




```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = True)
```

    Epoch 1/50
    180/180 [==============================] - 4s 15ms/step - loss: 0.3288 - accuracy: 0.9468 - val_loss: 0.1630 - val_accuracy: 0.9831
    Epoch 2/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.1249 - accuracy: 0.9798 - val_loss: 0.0922 - val_accuracy: 0.9869
    Epoch 3/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0824 - accuracy: 0.9859 - val_loss: 0.0658 - val_accuracy: 0.9893
    Epoch 4/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0666 - accuracy: 0.9866 - val_loss: 0.0551 - val_accuracy: 0.9876
    Epoch 5/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0573 - accuracy: 0.9874 - val_loss: 0.0429 - val_accuracy: 0.9933
    Epoch 6/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0494 - accuracy: 0.9889 - val_loss: 0.0427 - val_accuracy: 0.9918
    Epoch 7/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0456 - accuracy: 0.9891 - val_loss: 0.0335 - val_accuracy: 0.9938
    Epoch 8/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0443 - accuracy: 0.9896 - val_loss: 0.0384 - val_accuracy: 0.9927
    Epoch 9/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0392 - accuracy: 0.9895 - val_loss: 0.0271 - val_accuracy: 0.9951
    Epoch 10/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0391 - accuracy: 0.9899 - val_loss: 0.0249 - val_accuracy: 0.9944
    Epoch 11/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0343 - accuracy: 0.9911 - val_loss: 0.0237 - val_accuracy: 0.9962
    Epoch 12/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0331 - accuracy: 0.9917 - val_loss: 0.0272 - val_accuracy: 0.9949
    Epoch 13/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0344 - accuracy: 0.9905 - val_loss: 0.0249 - val_accuracy: 0.9929
    Epoch 14/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0324 - accuracy: 0.9905 - val_loss: 0.0230 - val_accuracy: 0.9957
    Epoch 15/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0309 - accuracy: 0.9924 - val_loss: 0.0222 - val_accuracy: 0.9953
    Epoch 16/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0283 - accuracy: 0.9914 - val_loss: 0.0234 - val_accuracy: 0.9953
    Epoch 17/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0257 - accuracy: 0.9929 - val_loss: 0.0217 - val_accuracy: 0.9960
    Epoch 18/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0275 - accuracy: 0.9922 - val_loss: 0.0183 - val_accuracy: 0.9951
    Epoch 19/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0274 - accuracy: 0.9924 - val_loss: 0.0180 - val_accuracy: 0.9967
    Epoch 20/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0248 - accuracy: 0.9930 - val_loss: 0.0190 - val_accuracy: 0.9956
    Epoch 21/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0240 - accuracy: 0.9935 - val_loss: 0.0191 - val_accuracy: 0.9942
    Epoch 22/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0273 - accuracy: 0.9925 - val_loss: 0.0160 - val_accuracy: 0.9969
    Epoch 23/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0249 - accuracy: 0.9921 - val_loss: 0.0195 - val_accuracy: 0.9956
    Epoch 24/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0239 - accuracy: 0.9927 - val_loss: 0.0146 - val_accuracy: 0.9956
    Epoch 25/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0267 - accuracy: 0.9919 - val_loss: 0.0172 - val_accuracy: 0.9969
    Epoch 26/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0213 - accuracy: 0.9939 - val_loss: 0.0149 - val_accuracy: 0.9969
    Epoch 27/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0212 - accuracy: 0.9940 - val_loss: 0.0135 - val_accuracy: 0.9969
    Epoch 28/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.0131 - val_accuracy: 0.9971
    Epoch 29/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0207 - accuracy: 0.9943 - val_loss: 0.0142 - val_accuracy: 0.9962
    Epoch 30/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0212 - accuracy: 0.9934 - val_loss: 0.0186 - val_accuracy: 0.9958
    Epoch 31/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.0099 - val_accuracy: 0.9980
    Epoch 32/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0196 - accuracy: 0.9942 - val_loss: 0.0094 - val_accuracy: 0.9984
    Epoch 33/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0195 - accuracy: 0.9935 - val_loss: 0.0106 - val_accuracy: 0.9980
    Epoch 34/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0185 - accuracy: 0.9945 - val_loss: 0.0093 - val_accuracy: 0.9982
    Epoch 35/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.0083 - val_accuracy: 0.9978
    Epoch 36/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0182 - accuracy: 0.9944 - val_loss: 0.0116 - val_accuracy: 0.9964
    Epoch 37/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0182 - accuracy: 0.9944 - val_loss: 0.0112 - val_accuracy: 0.9978
    Epoch 38/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.0097 - val_accuracy: 0.9984
    Epoch 39/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0162 - accuracy: 0.9949 - val_loss: 0.0089 - val_accuracy: 0.9987
    Epoch 40/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.0140 - val_accuracy: 0.9976
    Epoch 41/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0103 - val_accuracy: 0.9971
    Epoch 42/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0163 - accuracy: 0.9948 - val_loss: 0.0058 - val_accuracy: 0.9984
    Epoch 43/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0155 - accuracy: 0.9958 - val_loss: 0.0074 - val_accuracy: 0.9993
    Epoch 44/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.0076 - val_accuracy: 0.9982
    Epoch 45/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0177 - accuracy: 0.9943 - val_loss: 0.0117 - val_accuracy: 0.9973
    Epoch 46/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0140 - accuracy: 0.9957 - val_loss: 0.0076 - val_accuracy: 0.9984
    Epoch 47/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0173 - accuracy: 0.9942 - val_loss: 0.0080 - val_accuracy: 0.9987
    Epoch 48/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0132 - accuracy: 0.9955 - val_loss: 0.0113 - val_accuracy: 0.9978
    Epoch 49/50
    180/180 [==============================] - 3s 14ms/step - loss: 0.0136 - accuracy: 0.9955 - val_loss: 0.0077 - val_accuracy: 0.9982
    Epoch 50/50
    180/180 [==============================] - 3s 16ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.0048 - val_accuracy: 0.9989



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f6e2a890150>




    
![png](/images/Untitled1_files/Untitled1_41_1.png)
    


This model which uses both the title and texts gives the best accuracy. So from our three models, the most effective way to detect fake news is to look at both the title and the text. However, using both inputs is a lot slower than just using the title and as we see from the result above, just using the title as input gives very good result. So if we want our model to run more quickly, using model 1 is the better choice.

Now, we import the test data.


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test = pd.read_csv(test_url)
```


```python
test = make_dataset(test)
```


```python
model.evaluate(test)
```

    225/225 [==============================] - 2s 9ms/step - loss: 0.1086 - accuracy: 0.9807





    [0.10859087854623795, 0.980711817741394]



Using Model 3, the accuracy is approximately 98 on the testing data.

Now we visualize the embedding by using dimensionality reduction.




```python
weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
from plotly.io import write_html
write_html(fig, "1.html")
```


{% include 3-2.html %}

We can see from the plot that fake news tend to have words such as "reportedly","wonder", "radical","seems","terror","apparently","21st","21wire" while regular news tend to have words such as "congressional", "statements","reporters","proposals", "newspaper","spokesman","citing".

{::options parse_block_html="true" /}
<div class="got-help">
I learned from one peer that '21stâ€™ and â€˜21wireâ€™ refers to a website called 21st Century Wire, sometimes also called 21Wire which is a very unreliable website.
</div>
{::options parse_block_html="false" /}



```python

```
