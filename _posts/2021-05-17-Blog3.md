---
layout: post
title: Blog3
---

### When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?

We will answer this question by looking at three different machine learning models. We will create these neural network models using the Tensorflow package. A neural network is a series of algorithms that tries to recognize underlying relationships in a set of data. The networks are composed of several layers. Each layer has many units where computation happens. The unit multiplies the input data with a set of weights which is the process of assigning significance to inputs with regard to the task the algorithm is trying to learn. These input-weight products are summed and then the sum is passed through a activation function which determines to what extent the information should progress further through the network to affect the ultimate result.


```python
import numpy as np
import pandas as pd
import tensorflow as tf
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

# requires update to tensorflow 2.4
# >>> conda activate PIC16B
# >>> pip install tensorflow==2.4
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# for embedding viz
import plotly.express as px 
import plotly.io as pio
pio.templates.default = "plotly_white"

train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url)
```

We first take a look at the dataset. The title column is the title of the article and the text column is the full article text. The fake column is 0 if the article is true and 1 if the article contains fake news.


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>




```python
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Package stopwords is already up-to-date!


We need to first take away the stopwords, such as "it, the, is, of, to". Then we need to create a dataset with two inputs and one output. The inputs are title and text, and the output is the fake column.


```python
stop = stopwords.words('english')
def make_dataset(df):
  '''
  Turn the texts into lowercase letters and remove punctuations 
  in order to fully remove the stopwords. 
  Make the dataset and batch the dataset 
  Return the batched dataset

  '''
  df["title"] = df["title"].str.lower()
  df["text"] = df["text"].str.lower()
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['title'] = df['title'].str.replace(r'[^\w\s]+', '')
  df['text'] = df['text'].str.replace(r'[^\w\s]+', '') 
  df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
  data = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : df[["title"]], 
            "text" : df["text"]
        }, 
        {
            "fake" : df[["fake"]]
        }
    )
    )
  
  data = data.batch(100)
  return data
   

```


```python
data = make_dataset(df)
```

Now we set 80% as training data and 20% as validation data.


```python
data = data.shuffle(buffer_size = len(data))
train_size = int(0.8*len(data))
val_size   = int(0.2*len(data))

train = data.take(train_size)
val   = data.skip(train_size)
len(train), len(val)
```




    (180, 45)



Next we perform vectorization. Vectorization is the process of representing text as a vector. Here, we'll replace each word by its frequency rank in the data. Each entry in the vector is an integer that represents the frequency rank. For example 200, represents the word is the 200th most common word in the data set. 

In the first model, we only consider the title.


```python
size_vocabulary = 2000 # only the top distinct words will be tracked

vectorize_layer = TextVectorization(
    max_tokens=size_vocabulary, 
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer.adapt(train.map(lambda x, y: x["title"]))#each title will be a vector of length 500
```

Now we specify the title input as the predictor data. 
The shape parameter is the shape of a single item of data. The title column contains just one entry, so the shape is (1,) (a tuple of length 1). We set the name to "title" which will be used later.
The dtype specifies the type of data contained in the input tensors. In this case, the title is a string.


```python
# title input
title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

```

Now we construct the first model. The model has a embedding layer and dropout layers and a dense layer.




```python
title_features = vectorize_layer(title_input)
embedding = layers.Embedding(size_vocabulary, 10, name = "embedding")
title_features = embedding(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
output = layers.Dense(2, name = "fake")(title_features)
```


```python
model = keras.Model(
    inputs = [title_input],
    outputs = output
)
```


```python
model.summary()
```

    Model: "model"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    title (InputLayer)           [(None, 1)]               0         
    _________________________________________________________________
    text_vectorization_3 (TextVe (None, 500)               0         
    _________________________________________________________________
    embedding (Embedding)        (None, 500, 10)           20000     
    _________________________________________________________________
    dropout (Dropout)            (None, 500, 10)           0         
    _________________________________________________________________
    global_average_pooling1d (Gl (None, 10)                0         
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 10)                0         
    _________________________________________________________________
    dense (Dense)                (None, 32)                352       
    _________________________________________________________________
    fake (Dense)                 (None, 2)                 66        
    =================================================================
    Total params: 20,418
    Trainable params: 20,418
    Non-trainable params: 0
    _________________________________________________________________



```python
keras.utils.plot_model(model)
```




    
![png](/images/Untitled0_files/Untitled0_20_0.png)
    




```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = True)
```

    Epoch 1/50


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:
    
    Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
    


    180/180 [==============================] - 5s 8ms/step - loss: 0.6922 - accuracy: 0.5212 - val_loss: 0.6907 - val_accuracy: 0.5226
    Epoch 2/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.6891 - accuracy: 0.5246 - val_loss: 0.6678 - val_accuracy: 0.5716
    Epoch 3/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.6441 - accuracy: 0.6823 - val_loss: 0.5288 - val_accuracy: 0.8467
    Epoch 4/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.4848 - accuracy: 0.8389 - val_loss: 0.3665 - val_accuracy: 0.8760
    Epoch 5/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.3473 - accuracy: 0.8784 - val_loss: 0.2699 - val_accuracy: 0.9053
    Epoch 6/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.2723 - accuracy: 0.9000 - val_loss: 0.2314 - val_accuracy: 0.9164
    Epoch 7/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.2361 - accuracy: 0.9088 - val_loss: 0.2049 - val_accuracy: 0.9242
    Epoch 8/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.2135 - accuracy: 0.9165 - val_loss: 0.1849 - val_accuracy: 0.9287
    Epoch 9/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1930 - accuracy: 0.9266 - val_loss: 0.1706 - val_accuracy: 0.9333
    Epoch 10/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1737 - accuracy: 0.9343 - val_loss: 0.1579 - val_accuracy: 0.9368
    Epoch 11/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1631 - accuracy: 0.9362 - val_loss: 0.1646 - val_accuracy: 0.9311
    Epoch 12/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1653 - accuracy: 0.9355 - val_loss: 0.1583 - val_accuracy: 0.9357
    Epoch 13/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1540 - accuracy: 0.9406 - val_loss: 0.1542 - val_accuracy: 0.9344
    Epoch 14/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1537 - accuracy: 0.9410 - val_loss: 0.1420 - val_accuracy: 0.9433
    Epoch 15/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1448 - accuracy: 0.9447 - val_loss: 0.1330 - val_accuracy: 0.9498
    Epoch 16/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1464 - accuracy: 0.9432 - val_loss: 0.1418 - val_accuracy: 0.9424
    Epoch 17/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1425 - accuracy: 0.9447 - val_loss: 0.1234 - val_accuracy: 0.9528
    Epoch 18/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1382 - accuracy: 0.9475 - val_loss: 0.1175 - val_accuracy: 0.9560
    Epoch 19/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1369 - accuracy: 0.9455 - val_loss: 0.1132 - val_accuracy: 0.9566
    Epoch 20/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1300 - accuracy: 0.9484 - val_loss: 0.1227 - val_accuracy: 0.9518
    Epoch 21/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1338 - accuracy: 0.9471 - val_loss: 0.1178 - val_accuracy: 0.9573
    Epoch 22/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1296 - accuracy: 0.9496 - val_loss: 0.1190 - val_accuracy: 0.9551
    Epoch 23/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1240 - accuracy: 0.9529 - val_loss: 0.1160 - val_accuracy: 0.9562
    Epoch 24/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1236 - accuracy: 0.9537 - val_loss: 0.1043 - val_accuracy: 0.9602
    Epoch 25/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1284 - accuracy: 0.9476 - val_loss: 0.1137 - val_accuracy: 0.9591
    Epoch 26/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1282 - accuracy: 0.9491 - val_loss: 0.1124 - val_accuracy: 0.9556
    Epoch 27/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1321 - accuracy: 0.9490 - val_loss: 0.1210 - val_accuracy: 0.9500
    Epoch 28/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1313 - accuracy: 0.9498 - val_loss: 0.1169 - val_accuracy: 0.9540
    Epoch 29/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1181 - accuracy: 0.9549 - val_loss: 0.1150 - val_accuracy: 0.9549
    Epoch 30/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1228 - accuracy: 0.9524 - val_loss: 0.1018 - val_accuracy: 0.9613
    Epoch 31/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1166 - accuracy: 0.9563 - val_loss: 0.1119 - val_accuracy: 0.9538
    Epoch 32/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1176 - accuracy: 0.9563 - val_loss: 0.1134 - val_accuracy: 0.9549
    Epoch 33/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1171 - accuracy: 0.9559 - val_loss: 0.1105 - val_accuracy: 0.9566
    Epoch 34/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1137 - accuracy: 0.9568 - val_loss: 0.1086 - val_accuracy: 0.9567
    Epoch 35/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1111 - accuracy: 0.9553 - val_loss: 0.1120 - val_accuracy: 0.9558
    Epoch 36/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1157 - accuracy: 0.9552 - val_loss: 0.0966 - val_accuracy: 0.9607
    Epoch 37/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1138 - accuracy: 0.9575 - val_loss: 0.1115 - val_accuracy: 0.9582
    Epoch 38/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1134 - accuracy: 0.9561 - val_loss: 0.1073 - val_accuracy: 0.9600
    Epoch 39/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1132 - accuracy: 0.9568 - val_loss: 0.1139 - val_accuracy: 0.9511
    Epoch 40/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1167 - accuracy: 0.9533 - val_loss: 0.0990 - val_accuracy: 0.9644
    Epoch 41/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1034 - accuracy: 0.9595 - val_loss: 0.0960 - val_accuracy: 0.9636
    Epoch 42/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1142 - accuracy: 0.9554 - val_loss: 0.1021 - val_accuracy: 0.9616
    Epoch 43/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1161 - accuracy: 0.9546 - val_loss: 0.1162 - val_accuracy: 0.9542
    Epoch 44/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1226 - accuracy: 0.9537 - val_loss: 0.0959 - val_accuracy: 0.9658
    Epoch 45/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1061 - accuracy: 0.9569 - val_loss: 0.0923 - val_accuracy: 0.9609
    Epoch 46/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1081 - accuracy: 0.9594 - val_loss: 0.1085 - val_accuracy: 0.9556
    Epoch 47/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1088 - accuracy: 0.9601 - val_loss: 0.1030 - val_accuracy: 0.9580
    Epoch 48/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1107 - accuracy: 0.9570 - val_loss: 0.1089 - val_accuracy: 0.9580
    Epoch 49/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1070 - accuracy: 0.9579 - val_loss: 0.0962 - val_accuracy: 0.9653
    Epoch 50/50
    180/180 [==============================] - 1s 7ms/step - loss: 0.1060 - accuracy: 0.9623 - val_loss: 0.0992 - val_accuracy: 0.9633



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f78e015f4d0>




    
![png](/images/Untitled0_files/Untitled0_23_1.png)
    


This model is able to score approximately 96% validation accuracy. We see from the above graph that the validation accuracy is slightly higher that the training accuracy. This is due to the fact that we have dropout layers in the model. During training, the algorithm dropped out 20% of the units which prevents overfitting. But these units are used during validation, so it can happen that the validation scores can be higher.

Now we create the second model that only uses the text which follows the same process as before.


```python
vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
# inputs

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```


```python
text_features = vectorize_layer(text_input)
text_features = embedding(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
output = layers.Dense(2, name = "fake")(text_features)
```


```python
model = keras.Model(
    inputs = [text_input],
    outputs = output
)
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
history = model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = True)
```

    Epoch 1/50


    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:595: UserWarning:
    
    Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
    


    180/180 [==============================] - 3s 13ms/step - loss: 0.6885 - accuracy: 0.5651 - val_loss: 0.6493 - val_accuracy: 0.6584
    Epoch 2/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.5940 - accuracy: 0.7730 - val_loss: 0.3372 - val_accuracy: 0.9273
    Epoch 3/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.3257 - accuracy: 0.8975 - val_loss: 0.1957 - val_accuracy: 0.9521
    Epoch 4/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.2151 - accuracy: 0.9288 - val_loss: 0.1605 - val_accuracy: 0.9587
    Epoch 5/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.1741 - accuracy: 0.9425 - val_loss: 0.1317 - val_accuracy: 0.9681
    Epoch 6/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.1434 - accuracy: 0.9563 - val_loss: 0.1144 - val_accuracy: 0.9707
    Epoch 7/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.1302 - accuracy: 0.9616 - val_loss: 0.1036 - val_accuracy: 0.9753
    Epoch 8/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.1253 - accuracy: 0.9639 - val_loss: 0.0969 - val_accuracy: 0.9753
    Epoch 9/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.1068 - accuracy: 0.9691 - val_loss: 0.0872 - val_accuracy: 0.9760
    Epoch 10/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0995 - accuracy: 0.9699 - val_loss: 0.0825 - val_accuracy: 0.9731
    Epoch 11/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.1029 - accuracy: 0.9713 - val_loss: 0.0731 - val_accuracy: 0.9842
    Epoch 12/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0907 - accuracy: 0.9743 - val_loss: 0.0692 - val_accuracy: 0.9816
    Epoch 13/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0864 - accuracy: 0.9737 - val_loss: 0.0703 - val_accuracy: 0.9816
    Epoch 14/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0824 - accuracy: 0.9758 - val_loss: 0.0668 - val_accuracy: 0.9831
    Epoch 15/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0784 - accuracy: 0.9785 - val_loss: 0.0585 - val_accuracy: 0.9842
    Epoch 16/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0741 - accuracy: 0.9780 - val_loss: 0.0614 - val_accuracy: 0.9844
    Epoch 17/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0730 - accuracy: 0.9790 - val_loss: 0.0594 - val_accuracy: 0.9851
    Epoch 18/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0702 - accuracy: 0.9805 - val_loss: 0.0569 - val_accuracy: 0.9849
    Epoch 19/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0711 - accuracy: 0.9787 - val_loss: 0.0574 - val_accuracy: 0.9851
    Epoch 20/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0689 - accuracy: 0.9809 - val_loss: 0.0524 - val_accuracy: 0.9888
    Epoch 21/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0633 - accuracy: 0.9826 - val_loss: 0.0502 - val_accuracy: 0.9896
    Epoch 22/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0594 - accuracy: 0.9832 - val_loss: 0.0472 - val_accuracy: 0.9867
    Epoch 23/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0604 - accuracy: 0.9820 - val_loss: 0.0434 - val_accuracy: 0.9908
    Epoch 24/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0549 - accuracy: 0.9852 - val_loss: 0.0397 - val_accuracy: 0.9913
    Epoch 25/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0532 - accuracy: 0.9851 - val_loss: 0.0385 - val_accuracy: 0.9912
    Epoch 26/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0548 - accuracy: 0.9838 - val_loss: 0.0396 - val_accuracy: 0.9893
    Epoch 27/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0531 - accuracy: 0.9854 - val_loss: 0.0463 - val_accuracy: 0.9898
    Epoch 28/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0543 - accuracy: 0.9840 - val_loss: 0.0410 - val_accuracy: 0.9879
    Epoch 29/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0515 - accuracy: 0.9835 - val_loss: 0.0339 - val_accuracy: 0.9922
    Epoch 30/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0544 - accuracy: 0.9832 - val_loss: 0.0372 - val_accuracy: 0.9911
    Epoch 31/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0481 - accuracy: 0.9866 - val_loss: 0.0306 - val_accuracy: 0.9929
    Epoch 32/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0460 - accuracy: 0.9863 - val_loss: 0.0315 - val_accuracy: 0.9908
    Epoch 33/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0454 - accuracy: 0.9864 - val_loss: 0.0349 - val_accuracy: 0.9904
    Epoch 34/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0446 - accuracy: 0.9873 - val_loss: 0.0287 - val_accuracy: 0.9922
    Epoch 35/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0454 - accuracy: 0.9868 - val_loss: 0.0291 - val_accuracy: 0.9922
    Epoch 36/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0429 - accuracy: 0.9868 - val_loss: 0.0297 - val_accuracy: 0.9930
    Epoch 37/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0420 - accuracy: 0.9853 - val_loss: 0.0307 - val_accuracy: 0.9924
    Epoch 38/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0416 - accuracy: 0.9870 - val_loss: 0.0324 - val_accuracy: 0.9931
    Epoch 39/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0360 - accuracy: 0.9888 - val_loss: 0.0244 - val_accuracy: 0.9926
    Epoch 40/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0360 - accuracy: 0.9889 - val_loss: 0.0339 - val_accuracy: 0.9940
    Epoch 41/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0375 - accuracy: 0.9877 - val_loss: 0.0204 - val_accuracy: 0.9944
    Epoch 42/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0417 - accuracy: 0.9879 - val_loss: 0.0255 - val_accuracy: 0.9937
    Epoch 43/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0349 - accuracy: 0.9887 - val_loss: 0.0202 - val_accuracy: 0.9962
    Epoch 44/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0361 - accuracy: 0.9886 - val_loss: 0.0252 - val_accuracy: 0.9940
    Epoch 45/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0369 - accuracy: 0.9887 - val_loss: 0.0259 - val_accuracy: 0.9946
    Epoch 46/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0338 - accuracy: 0.9891 - val_loss: 0.0219 - val_accuracy: 0.9949
    Epoch 47/50
    180/180 [==============================] - 2s 12ms/step - loss: 0.0342 - accuracy: 0.9889 - val_loss: 0.0283 - val_accuracy: 0.9924
    Epoch 48/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0353 - accuracy: 0.9891 - val_loss: 0.0214 - val_accuracy: 0.9938
    Epoch 49/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0342 - accuracy: 0.9890 - val_loss: 0.0162 - val_accuracy: 0.9973
    Epoch 50/50
    180/180 [==============================] - 2s 13ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 0.0190 - val_accuracy: 0.9960



```python
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f78969c6050>




    
![png](/images/Untitled0_files/Untitled0_29_1.png)
    


The validation score reached 99.

Next we create the third model which uses both the title and the text.


```python
main = layers.concatenate([title_features, text_features], axis = 1)
```


```python
output = layers.Dense(2, name = "fake")(main)
```


```python
model = keras.Model(
    inputs = [title_input, text_input],
    outputs = output
)
model.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
    Layer (type)                    Output Shape         Param #     Connected to                     
    ==================================================================================================
    title (InputLayer)              [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text (InputLayer)               [(None, 1)]          0                                            
    __________________________________________________________________________________________________
    text_vectorization_3 (TextVecto (None, 500)          0           title[0][0]                      
                                                                     text[0][0]                       
    __________________________________________________________________________________________________
    embedding (Embedding)           (None, 500, 10)      20000       text_vectorization_3[0][0]       
                                                                     text_vectorization_3[1][0]       
    __________________________________________________________________________________________________
    dropout (Dropout)               (None, 500, 10)      0           embedding[0][0]                  
    __________________________________________________________________________________________________
    dropout_2 (Dropout)             (None, 500, 10)      0           embedding[1][0]                  
    __________________________________________________________________________________________________
    global_average_pooling1d (Globa (None, 10)           0           dropout[0][0]                    
    __________________________________________________________________________________________________
    global_average_pooling1d_1 (Glo (None, 10)           0           dropout_2[0][0]                  
    __________________________________________________________________________________________________
    dropout_1 (Dropout)             (None, 10)           0           global_average_pooling1d[0][0]   
    __________________________________________________________________________________________________
    dropout_3 (Dropout)             (None, 10)           0           global_average_pooling1d_1[0][0] 
    __________________________________________________________________________________________________
    dense (Dense)                   (None, 32)           352         dropout_1[0][0]                  
    __________________________________________________________________________________________________
    dense_1 (Dense)                 (None, 32)           352         dropout_3[0][0]                  
    __________________________________________________________________________________________________
    concatenate (Concatenate)       (None, 64)           0           dense[0][0]                      
                                                                     dense_1[0][0]                    
    __________________________________________________________________________________________________
    fake (Dense)                    (None, 2)            130         concatenate[0][0]                
    ==================================================================================================
    Total params: 20,834
    Trainable params: 20,834
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
keras.utils.plot_model(model)
```




    
![png](/images/Untitled0_files/Untitled0_35_0.png)
    




```python
model.compile(optimizer = "adam",
              loss = losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy']
)
```


```python
history = model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = True)
```

    Epoch 1/50
    180/180 [==============================] - 4s 17ms/step - loss: 0.4050 - accuracy: 0.9703 - val_loss: 0.1356 - val_accuracy: 0.9931
    Epoch 2/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.1351 - accuracy: 0.9742 - val_loss: 0.0806 - val_accuracy: 0.9916
    Epoch 3/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0831 - accuracy: 0.9867 - val_loss: 0.0559 - val_accuracy: 0.9901
    Epoch 4/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0657 - accuracy: 0.9858 - val_loss: 0.0491 - val_accuracy: 0.9911
    Epoch 5/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0563 - accuracy: 0.9870 - val_loss: 0.0435 - val_accuracy: 0.9931
    Epoch 6/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0480 - accuracy: 0.9892 - val_loss: 0.0427 - val_accuracy: 0.9931
    Epoch 7/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0454 - accuracy: 0.9893 - val_loss: 0.0305 - val_accuracy: 0.9936
    Epoch 8/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0428 - accuracy: 0.9891 - val_loss: 0.0318 - val_accuracy: 0.9939
    Epoch 9/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0389 - accuracy: 0.9900 - val_loss: 0.0355 - val_accuracy: 0.9911
    Epoch 10/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0384 - accuracy: 0.9898 - val_loss: 0.0331 - val_accuracy: 0.9881
    Epoch 11/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0336 - accuracy: 0.9912 - val_loss: 0.0308 - val_accuracy: 0.9940
    Epoch 12/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0327 - accuracy: 0.9909 - val_loss: 0.0236 - val_accuracy: 0.9951
    Epoch 13/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0307 - accuracy: 0.9922 - val_loss: 0.0252 - val_accuracy: 0.9931
    Epoch 14/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0316 - accuracy: 0.9921 - val_loss: 0.0208 - val_accuracy: 0.9951
    Epoch 15/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0285 - accuracy: 0.9935 - val_loss: 0.0257 - val_accuracy: 0.9947
    Epoch 16/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0303 - accuracy: 0.9919 - val_loss: 0.0227 - val_accuracy: 0.9964
    Epoch 17/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0268 - accuracy: 0.9927 - val_loss: 0.0219 - val_accuracy: 0.9964
    Epoch 18/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0269 - accuracy: 0.9916 - val_loss: 0.0175 - val_accuracy: 0.9976
    Epoch 19/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0276 - accuracy: 0.9929 - val_loss: 0.0188 - val_accuracy: 0.9960
    Epoch 20/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0292 - accuracy: 0.9922 - val_loss: 0.0222 - val_accuracy: 0.9953
    Epoch 21/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0282 - accuracy: 0.9919 - val_loss: 0.0129 - val_accuracy: 0.9973
    Epoch 22/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0229 - accuracy: 0.9933 - val_loss: 0.0153 - val_accuracy: 0.9962
    Epoch 23/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0262 - accuracy: 0.9923 - val_loss: 0.0110 - val_accuracy: 0.9980
    Epoch 24/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0219 - accuracy: 0.9938 - val_loss: 0.0165 - val_accuracy: 0.9966
    Epoch 25/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0226 - accuracy: 0.9929 - val_loss: 0.0164 - val_accuracy: 0.9962
    Epoch 26/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0199 - accuracy: 0.9939 - val_loss: 0.0173 - val_accuracy: 0.9962
    Epoch 27/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0247 - accuracy: 0.9936 - val_loss: 0.0149 - val_accuracy: 0.9969
    Epoch 28/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0249 - accuracy: 0.9938 - val_loss: 0.0164 - val_accuracy: 0.9964
    Epoch 29/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0238 - accuracy: 0.9936 - val_loss: 0.0138 - val_accuracy: 0.9973
    Epoch 30/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0198 - accuracy: 0.9940 - val_loss: 0.0165 - val_accuracy: 0.9969
    Epoch 31/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0238 - accuracy: 0.9928 - val_loss: 0.0123 - val_accuracy: 0.9973
    Epoch 32/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.0125 - val_accuracy: 0.9980
    Epoch 33/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0228 - accuracy: 0.9935 - val_loss: 0.0182 - val_accuracy: 0.9975
    Epoch 34/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0194 - accuracy: 0.9943 - val_loss: 0.0102 - val_accuracy: 0.9980
    Epoch 35/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0167 - accuracy: 0.9951 - val_loss: 0.0222 - val_accuracy: 0.9929
    Epoch 36/50
    180/180 [==============================] - 3s 16ms/step - loss: 0.0222 - accuracy: 0.9926 - val_loss: 0.0119 - val_accuracy: 0.9976
    Epoch 37/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0179 - accuracy: 0.9955 - val_loss: 0.0113 - val_accuracy: 0.9978
    Epoch 38/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.0126 - val_accuracy: 0.9973
    Epoch 39/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.0126 - val_accuracy: 0.9969
    Epoch 40/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0165 - accuracy: 0.9945 - val_loss: 0.0094 - val_accuracy: 0.9984
    Epoch 41/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0184 - accuracy: 0.9933 - val_loss: 0.0093 - val_accuracy: 0.9982
    Epoch 42/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0154 - accuracy: 0.9957 - val_loss: 0.0118 - val_accuracy: 0.9976
    Epoch 43/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0187 - accuracy: 0.9944 - val_loss: 0.0124 - val_accuracy: 0.9978
    Epoch 44/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0165 - accuracy: 0.9955 - val_loss: 0.0099 - val_accuracy: 0.9980
    Epoch 45/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0154 - accuracy: 0.9946 - val_loss: 0.0102 - val_accuracy: 0.9971
    Epoch 46/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0170 - accuracy: 0.9942 - val_loss: 0.0092 - val_accuracy: 0.9984
    Epoch 47/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.0073 - val_accuracy: 0.9980
    Epoch 48/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0147 - accuracy: 0.9947 - val_loss: 0.0056 - val_accuracy: 0.9989
    Epoch 49/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0168 - accuracy: 0.9940 - val_loss: 0.0066 - val_accuracy: 0.9987
    Epoch 50/50
    180/180 [==============================] - 3s 15ms/step - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0087 - val_accuracy: 0.9982



```python
from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f78a8099590>




    
![png](/images/Untitled0_files/Untitled0_38_1.png)
    


This model which uses both the title and texts gives the best accuracy. So from our three models, the most effictive way to detect fake news is to look at both the title and the text.

We import the test data.


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test = pd.read_csv(test_url)
```


```python
test = make_dataset(test)
```


```python
model.evaluate(test)
```

    225/225 [==============================] - 2s 9ms/step - loss: 0.1238 - accuracy: 0.9772





    [0.12380985915660858, 0.9772372841835022]



The accuracy is approximately 98 on the testing data.

Now we look at the embedding.


```python
weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)

embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show()
```


{% include 3-1.html %}


We can see from the plot that fake news tend to have words such as "reportedly","wonder", "radical","seems" while regular news tend to have words such as "responsibility", "congressional", "statements","reporters","proposals", "newspaper".


```python

```
